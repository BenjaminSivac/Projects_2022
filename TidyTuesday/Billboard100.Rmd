---
title: "Tidy Tuesday: Billboard 100"
author: "Benjamin Sivac"
date: "`r Sys.Date()`"
output: 
  rmarkdown::github_document
---
```{r, include=FALSE}
hook_output <- knitr::knit_hooks$get("output")
```

# Introduction
This EDA is done by following a video uploaded by an esteemed R-user named David
Robinsson. I figured it would be a great opportunity for me to see another 
analyst's approach to EDA and how he applies more advanced techniques. I'll aim
to describe and explain every step of the way to understand for both the reader 
and myself. 

## Data & Exploratory data analysis
The data comes from Data.World by way of Sean Miller, Billboard.com and Spotify. 
The Billboard Hot 100 is the music industry standard record chart in the United 
States for songs, published weekly by Billboard magazine. Chart rankings are 
based on sales (physical and digital), radio play, and online streaming in the
United States.

```{r Loading packages, message=FALSE, warning=FALSE}
library(tidyverse)
library(scales)
library(lubridate)
theme_set(theme_light())
df.bb100 <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-14/billboard.csv')
df.af <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-14/audio_features.csv')
```

Let's have our first look at the data.
```{r}
df.bb100 %>% glimpse()
```

This dataset already has a few interesting aggregated columns for each performer
and his/her respective song; weekly number, number of instances it has appeared 
on the billboard, positions on the chart, and a variable for cumulative weeks on
the chart. We will first fix the week_id variable as it needs to be converted 
into a date format.
```{r}
df.bb100 <- df.bb100 %>% 
  mutate(week = mdy(week_id)) %>% 
  select(-week_id)
```

Let's find out which songs have remained at number 1 position the longest.
```{r}
(longest_top_song <- df.bb100 %>% 
  filter(week_position==1) %>% 
  count(song_id, song, performer, sort= TRUE))
```

Let's find any patterns across time for these songs.
```{r}
df.bb100 %>% 
  semi_join(head(longest_top_song, 9), by = "song_id") %>% 
  ggplot(aes(week, week_position, group = instance)) +
  geom_line() +
  facet_wrap(~song, scales="free_x") +
  scale_y_reverse() +
  labs(x="Time",
       y="Billboard position",
       title = "Trajectories of #1 hits")
```

Most songs seem to rapidly reach the number 1 spot and remain for 3-4 months 
before slowly losing placements by the following 6 months. 

Let's evaluate the performers by how many songs and for how long they have been
at top 100 and number 1 respectively.
```{r}
summarize_songs <- function(tbl) {
  tbl %>% 
    summarize(total_weeks_on_top100 = n(),
            total_weeks_at_number1 = sum(week_position == 1),
            n_songs_top100 = n_distinct(song),
            n_songs_at_number1 = n_distinct(song[week_position==1]),
            .groups = "drop") %>% 
  arrange(desc(total_weeks_at_number1))
}

(by_performer <- df.bb100 %>% 
  group_by(performer) %>% 
  summarize_songs())
```

We can create a scatter plot to observe how many songs each performer got at 
number 1 out of their total number of songs in the top 100.
```{r}
by_performer %>% 
  arrange(desc(n_songs_top100)) %>% 
  ggplot(aes(n_songs_top100, n_songs_at_number1)) +
  geom_point() +
  labs(x = "# of songs on the billboard top 100",
       y="# of songs at #1") +
  geom_text(aes(label=performer), check_overlap = TRUE, vjust = 1, hjust = 1) +
  expand_limits(x=-10)
```

There's a pretty high concentration of performers within the range of 1 to 7 
songs at number 1 out of 10 to 50 at top100 songs.

It is also reasonable to observe stats by decade and also see which performer 
had the best numbers by each decade.
```{r}
(by_performer_decade <- df.bb100 %>% 
  group_by(performer, 
           decade = 10 * year(week) %/% 10) %>% 
  summarize_songs())
```

```{r}
by_performer_decade %>% 
  group_by(decade) %>% 
  slice_max(total_weeks_at_number1, n = 1)
```

We can visualize which performer had the most average weeks on the billboard per
5 year periods, by lumping together top 16 performers, filtering out the 
rest/"Other", and utilizing an area plot. We also use facet wrap to make it more
readable.
```{r}
df.bb100 %>% 
  mutate(performer_lumped = fct_lump(performer, 16)) %>% 
  count(performer_lumped,
        year = 5 * year(week) %/% 5) %>% 
  filter(performer_lumped != "Other") %>% 
  mutate(performer_lumped = fct_reorder(performer_lumped, year)) %>% 
  ggplot(aes(year, n/5, fill = performer_lumped)) + 
  geom_area() +
  facet_wrap(~performer_lumped, scales="free_y") +
  scale_fill_discrete(guide="none") +
  labs(x = "Year", 
       y = "Average weeks on Billboard Top 100 / year")
```

Next is to perform ML for predicting each song's popularity by number of weeks!

## Machine Learning
We'll utilise XGBoost to predict log_n_weeks by certain stats and characteristics 
found in the audio_features data, which we'll join together by an inner join.
```{r}
(by_song <- df.bb100 %>% 
  group_by(song_id) %>% 
  summarize(peak=max(week_position),
            week_started = min(week),
            n_weeks = n(),
            log_n_weeks = log2(n_weeks)))

songs_joined <- by_song %>%
  inner_join(df.af, by = "song_id") %>%
  filter(!is.na(spotify_track_id))
```

We perform an initial split to the data, creating a training set for estimating
parameters and a testing set for evaluating the machine learning method, across
3 blocks for cross validation which is rather few but we are only doing a quick test.
```{r message=FALSE, warning=FALSE}
library(tidymodels)
library(textrecipes)

set.seed(2022)
split <- initial_split(songs_joined) #Initial data split
train <- training(split) #training set for estimating parameters
test <- testing(split) #testing set for evaluating the ML method
folds <- vfold_cv(train, 3) # Divide the data into 3 blocks for cross validation
```

Onto preprocessing our recipe; we include all attributes to a song but also what
genre it has been listed as on spotify, and what month they hit the billboard.
Using step_mutate to adjust and clean the latter two variables to fit, while 
also converting the genres into individual variables through tokenization. 
```{r message=FALSE, warning=FALSE}
# Preprocessing "recipe"
xg_workflow <- recipe(log_n_weeks ~ danceability + energy + key + loudness + mode + speechiness +
         acousticness + instrumentalness + liveness + valence + tempo + time_signature +
         spotify_genre + week_started, data = train) %>% 
  step_mutate(month = month(week_started),
              # parse the genre
              spotify_genre = str_remove_all(spotify_genre, "\\['|'\\]")) %>% 
  step_rm(week_started) %>% 
  # create tokens as variables for each spotify genre
  step_tokenize(spotify_genre, token = "regex", options = list(pattern = "', '")) %>% 
  step_tokenfilter(spotify_genre, max_tokens = tune()) %>% 
  step_tf(spotify_genre) %>% 
  workflow(boost_tree("regression", # create a decision tree
                      mtry = tune(), # tunes hyperparameters (like booster parameters such as eta, gamma, and lambda)
                      trees=tune(),
                      learn_rate = .02))
tune <- xg_workflow %>% 
  tune_grid(folds,
            metrics = metric_set(rmse),
            grid = crossing(mtry = c(3, 5),
                            max_tokens = c(1,10,30),
                            trees = seq(25, 500, 25)))
autoplot(tune)
```

We find that the amount of predictors have no effect on the estimated rmse value
while the number of tokens do. 

We'll select the best hyperparameters and perform an estimated rmse value:
```{r}
xg_fit <- xg_workflow %>%
  finalize_workflow(select_best(tune)) %>%
  fit(train)

xg_fit %>%
  augment(test) %>%
  rmse(log_n_weeks, .pred)
```

Visualizing the fitted line through our test data.
```{r message=FALSE, warning=FALSE}
xg_fit %>%
  augment(test) %>%
  ggplot(aes(2 ^ .pred, 2 ^ log_n_weeks)) +
  geom_point(alpha = .25) +
  geom_smooth(method = "lm") +
  scale_x_log10() +
  scale_y_log10() +
  labs(x = "Predicted weeks on top 100",
       y = "Actual weeks on top 100")
```

## Interpreting the features

```{r}
importances <- xg_fit %>%
  extract_fit_engine() %>%
  xgb.importance(mod = .)
importances %>%
  mutate(Feature = fct_reorder(Feature, Gain)) %>%
  ggplot(aes(Gain, Feature)) +
  geom_col() +
  labs(x = "Importance")
```









